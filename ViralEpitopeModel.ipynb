{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on August 4th 2022\\n\\n@author: JJ\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on August 4th 2022\n",
    "\n",
    "@author: JJ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 14:45:01.990722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import random\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.regularizers import (\n",
    "    l2, \n",
    "    l1, \n",
    "    l1_l2\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import (\n",
    "    activations, \n",
    "    initializers, \n",
    "    regularizers, \n",
    "    constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from python files\n",
    "from model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "# Maximum number of threads to use for OpenMP parallel regions.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# Without setting below 2 environment variables, it didn't work for me. Thanks to @cjw85 \n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"4\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"4\"\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.threading.set_intra_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a well-defined initial state.\n",
    "random.seed\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    chi2\n",
    ")\n",
    "import keras_tuner as kt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open ('fulldata_pseudoseq_dictionary.pkl', 'rb') as f:\n",
    "#    pseudo_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('fulldata_pseudoseq_dictionary_converted.pkl', 'rb') as f:\n",
    "    pseudo_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pseudo_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    training_data = []\n",
    "    for allele in pseudo_seq.keys():\n",
    "        allele_data = pseudo_seq[allele]\n",
    "        random.shuffle(allele_data)\n",
    "        allele_data = np.array(allele_data)\n",
    "        training_data.extend(allele_data)\n",
    "    \n",
    "    [all_pep, all_mhc, all_target] = [[i[j] for i in training_data] for j in range(3)]\n",
    "    all_pep = np.array(all_pep)\n",
    "    all_mhc = np.array(all_mhc)\n",
    "    all_target = np.array(all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pep.shape)\n",
    "print(all_mhc.shape)\n",
    "print(all_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('/home/jjia1/jjia1/viralepitope/all_pep.npy', all_pep)\n",
    "#np.save('/home/jjia1/jjia1/viralepitope/all_mhc.npy', all_mhc)\n",
    "#np.save('/home/jjia1/jjia1/viralepitope/all_target.npy', all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/jjia1/jjia1/viralepitope/all_pep_converted.npy', all_pep)\n",
    "np.save('/home/jjia1/jjia1/viralepitope/all_mhc_converted.npy', all_mhc)\n",
    "np.save('/home/jjia1/jjia1/viralepitope/all_target_converted.npy', all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_pep, all_mhc, all_target = np.load('/home/jjia1/jjia1/viralepitope/all_pep.npy'), np.load('/home/jjia1/jjia1/viralepitope/all_mhc.npy'), np.load('/home/jjia1/jjia1/viralepitope/all_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pep, all_mhc, all_target = np.load('/home/jjia1/jjia1/viralepitope/all_pep_converted.npy'), np.load('/home/jjia1/jjia1/viralepitope/all_mhc_converted.npy'), np.load('/home/jjia1/jjia1/viralepitope/all_target_converted.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build 2 models\n",
    "#one model for the sequence, one model for the MHC names\n",
    "#integrate the outputs in a final dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(training_pep, training_mhc):\n",
    "        filters, kernel_size, fc1_size, fc2_size, fc3_size = 128, 3, 256, 64, 2\n",
    "        inputs_1 = Input(shape = (np.shape(training_pep[0])[0],20))\n",
    "        inputs_2 = Input(shape = (34,np.shape(training_mhc[0])[0]))\n",
    "        #Initial feature extraction using a convolutional layer\n",
    "        pep_conv = Conv1D(filters,kernel_size,padding = 'same',activation = 'relu',strides = 1)(inputs_1)\n",
    "        pep_maxpool = MaxPooling1D()(pep_conv)\n",
    "        mhc_conv_1 = Conv1D(filters,kernel_size,padding = 'same',activation = 'relu',strides = 1)(inputs_2)\n",
    "        mhc_maxpool_1 = MaxPooling1D()(mhc_conv_1)\n",
    "        #The convolutional module\n",
    "        mhc_conv_2 = Conv1D(filters,kernel_size,padding = 'same',activation = 'relu',strides = 1)(mhc_maxpool_1)\n",
    "        mhc_maxpool_2 = MaxPooling1D()(mhc_conv_2)\n",
    "        flat_pep_0 = Flatten()(pep_conv)\n",
    "        flat_pep_1 = Flatten()(pep_conv)\n",
    "        flat_pep_2 = Flatten()(pep_conv)\n",
    "        flat_mhc_0 = Flatten()(inputs_2)\n",
    "        flat_mhc_1 = Flatten()(mhc_maxpool_1)\n",
    "        flat_mhc_2 = Flatten()(mhc_maxpool_2)\n",
    "        cat_0 = Concatenate()([flat_pep_0, flat_mhc_0])\n",
    "        cat_1 = Concatenate()([flat_pep_1, flat_mhc_1])\n",
    "        cat_2 = Concatenate()([flat_pep_2, flat_mhc_2])        \n",
    "        fc1_0 = Dense(fc1_size,activation = \"relu\")(cat_0)\n",
    "        fc1_1 = Dense(fc1_size,activation = \"relu\")(cat_1)\n",
    "        fc1_2 = Dense(fc1_size,activation = \"relu\")(cat_2)\n",
    "        merge_1 = Concatenate()([fc1_0, fc1_1, fc1_2])\n",
    "        fc2 = Dense(fc2_size,activation = \"relu\")(merge_1)\n",
    "        fc3 = Dense(fc3_size,activation = \"relu\")(fc2)\n",
    "        #The attention module\n",
    "        mhc_attention_weights = Flatten()(TimeDistributed(Dense(1))(mhc_conv_1))\n",
    "        pep_attention_weights = Flatten()(TimeDistributed(Dense(1))(pep_conv))\n",
    "        mhc_attention_weights = Activation('softmax')(mhc_attention_weights)\n",
    "        pep_attention_weights = Activation('softmax')(pep_attention_weights)        \n",
    "        mhc_conv_permute = Permute((2,1))(mhc_conv_1)\n",
    "        pep_conv_permute = Permute((2,1))(pep_conv)\n",
    "        mhc_attention = Dot(-1)([mhc_conv_permute, mhc_attention_weights])\n",
    "        pep_attention = Dot(-1)([pep_conv_permute, pep_attention_weights])\n",
    "        #Concatenating the output of the two modules\n",
    "        merge_2 = Concatenate()([mhc_attention,pep_attention,fc3])\n",
    "        #Output of the model\n",
    "        out = Dense(1,activation = \"sigmoid\")(merge_2)\n",
    "        model = Model(inputs=[inputs_1, inputs_2],outputs=out)  \n",
    "        model.summary()\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pep, test_pep, train_mhc, test_mhc, train_target, test_target = train_test_split(all_pep, all_mhc, all_target, test_size = 0.2, stratify= all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(756347, 30, 20)\n",
      "(189087, 30, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_pep.shape)\n",
    "print(test_pep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"model/CV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1337)    \n",
    "allprobas_=np.array([]) \n",
    "all_labels=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "peptide (InputLayer)            [(None, 30, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mhc (InputLayer)                [(None, 34, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pep (Conv1D)              (None, 30, 128)      7808        peptide[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_mhc (Conv1D)              (None, 34, 128)      7808        mhc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 15, 128)      0           conv1_pep[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 17, 128)      0           conv1_mhc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 15, 128)      0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 17, 128)      0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1920)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2176)         0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 143)          33281       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1920)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 145)          33281       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2176)         0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_2 (attention_ (None, 128)          0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            1921        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_3 (attention_ (None, 128)          0           attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            2177        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 129)          0           attention_flatten_2[0][0]        \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 129)          0           attention_flatten_3[0][0]        \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            130         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            130         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2)            0           dense_6[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            3           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 86,539\n",
      "Trainable params: 86,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "peptide (InputLayer)            [(None, 30, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mhc (InputLayer)                [(None, 34, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pep (Conv1D)              (None, 30, 128)      7808        peptide[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_mhc (Conv1D)              (None, 34, 128)      7808        mhc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 15, 128)      0           conv1_pep[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 17, 128)      0           conv1_mhc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 15, 128)      0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 17, 128)      0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1920)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2176)         0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 143)          33281       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1920)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 145)          33281       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2176)         0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_2 (attention_ (None, 128)          0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            1921        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_flatten_3 (attention_ (None, 128)          0           attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            2177        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 129)          0           attention_flatten_2[0][0]        \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 129)          0           attention_flatten_3[0][0]        \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            130         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            130         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2)            0           dense_6[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            3           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 86,539\n",
      "Trainable params: 86,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 15:30:44.755345: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1452184800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1182/1182 [==============================] - 113s 94ms/step - loss: 0.2693 - accuracy: 0.9092 - auc: 0.7588 - val_loss: 0.2128 - val_accuracy: 0.9140 - val_auc: 0.8826\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.91398, saving model to model/CV/model_0.h5\n",
      "Epoch 2/100\n",
      "1182/1182 [==============================] - 107s 91ms/step - loss: 0.2167 - accuracy: 0.9142 - auc: 0.8734 - val_loss: 0.2055 - val_accuracy: 0.9153 - val_auc: 0.8931\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.91398 to 0.91532, saving model to model/CV/model_0.h5\n",
      "Epoch 3/100\n",
      "1182/1182 [==============================] - 104s 88ms/step - loss: 0.2117 - accuracy: 0.9146 - auc: 0.8823 - val_loss: 0.2022 - val_accuracy: 0.9166 - val_auc: 0.8968\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.91532 to 0.91661, saving model to model/CV/model_0.h5\n",
      "Epoch 4/100\n",
      "1182/1182 [==============================] - 101s 85ms/step - loss: 0.2092 - accuracy: 0.9156 - auc: 0.8870 - val_loss: 0.2013 - val_accuracy: 0.9159 - val_auc: 0.8996\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.91661\n",
      "Epoch 5/100\n",
      "1182/1182 [==============================] - 100s 85ms/step - loss: 0.2075 - accuracy: 0.9161 - auc: 0.8883 - val_loss: 0.1990 - val_accuracy: 0.9174 - val_auc: 0.9012\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.91661 to 0.91743, saving model to model/CV/model_0.h5\n",
      "Epoch 6/100\n",
      "1182/1182 [==============================] - 99s 84ms/step - loss: 0.2075 - accuracy: 0.9158 - auc: 0.8898 - val_loss: 0.1996 - val_accuracy: 0.9165 - val_auc: 0.9019\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.91743\n",
      "Epoch 7/100\n",
      "1182/1182 [==============================] - 103s 87ms/step - loss: 0.2064 - accuracy: 0.9163 - auc: 0.8901 - val_loss: 0.1975 - val_accuracy: 0.9182 - val_auc: 0.9030\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.91743 to 0.91823, saving model to model/CV/model_0.h5\n",
      "Epoch 8/100\n",
      "1182/1182 [==============================] - 104s 88ms/step - loss: 0.2049 - accuracy: 0.9165 - auc: 0.8918 - val_loss: 0.1971 - val_accuracy: 0.9183 - val_auc: 0.9031\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.91823 to 0.91834, saving model to model/CV/model_0.h5\n",
      "Epoch 9/100\n",
      "1182/1182 [==============================] - 102s 86ms/step - loss: 0.2040 - accuracy: 0.9166 - auc: 0.8936 - val_loss: 0.1965 - val_accuracy: 0.9193 - val_auc: 0.9039\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.91834 to 0.91932, saving model to model/CV/model_0.h5\n",
      "Epoch 10/100\n",
      "1182/1182 [==============================] - 104s 88ms/step - loss: 0.2039 - accuracy: 0.9168 - auc: 0.8939 - val_loss: 0.1973 - val_accuracy: 0.9176 - val_auc: 0.9044\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91932\n",
      "Epoch 11/100\n",
      "1182/1182 [==============================] - 102s 86ms/step - loss: 0.2036 - accuracy: 0.9173 - auc: 0.8935 - val_loss: 0.1963 - val_accuracy: 0.9195 - val_auc: 0.9054\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.91932 to 0.91950, saving model to model/CV/model_0.h5\n",
      "Epoch 12/100\n",
      "1182/1182 [==============================] - 103s 87ms/step - loss: 0.2025 - accuracy: 0.9180 - auc: 0.8938 - val_loss: 0.1966 - val_accuracy: 0.9182 - val_auc: 0.9042\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.91950\n",
      "Epoch 13/100\n",
      "1182/1182 [==============================] - 106s 89ms/step - loss: 0.2033 - accuracy: 0.9172 - auc: 0.8937 - val_loss: 0.1949 - val_accuracy: 0.9193 - val_auc: 0.9056\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.91950\n",
      "Epoch 14/100\n",
      "1182/1182 [==============================] - 103s 87ms/step - loss: 0.2035 - accuracy: 0.9171 - auc: 0.8941 - val_loss: 0.1962 - val_accuracy: 0.9182 - val_auc: 0.9053\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.91950\n",
      "Epoch 15/100\n",
      "1182/1182 [==============================] - 101s 85ms/step - loss: 0.2015 - accuracy: 0.9176 - auc: 0.8965 - val_loss: 0.1949 - val_accuracy: 0.9194 - val_auc: 0.9061\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.91950\n",
      "Epoch 16/100\n",
      "1182/1182 [==============================] - 103s 87ms/step - loss: 0.2020 - accuracy: 0.9177 - auc: 0.8960 - val_loss: 0.1943 - val_accuracy: 0.9201 - val_auc: 0.9059\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.91950 to 0.92011, saving model to model/CV/model_0.h5\n",
      "Epoch 17/100\n",
      "  82/1182 [=>............................] - ETA: 1:30 - loss: 0.1972 - accuracy: 0.9204 - auc: 0.8969"
     ]
    }
   ],
   "source": [
    "for i, (train, test) in enumerate(kfold.split(train_pep, train_target)):\n",
    "    training_pep = train_pep[train]\n",
    "    training_mhc = train_mhc[train]\n",
    "    training_target = train_target[train]\n",
    "    \n",
    "    validation_pep = train_pep[test]\n",
    "    validation_mhc = train_mhc[test]\n",
    "    validation_target = train_target[test]\n",
    "\n",
    "    mc = ModelCheckpoint(folder + '/model_' +str(i)+'.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    model = build_model(training_pep, training_mhc)\n",
    "    model.summary()\n",
    "\n",
    "    model.fit([training_pep,training_mhc], \n",
    "            training_target,\n",
    "            batch_size=512,\n",
    "            epochs = 100,\n",
    "            shuffle=True,\n",
    "            callbacks=[es, mc],\n",
    "            validation_data=([validation_pep,validation_mhc], validation_target),\n",
    "            verbose=1)\n",
    "\n",
    "    saved_model = load_model(folder + '/model_' +str(i)+'.h5')\n",
    "    probas_ = saved_model.predict([np.array(validation_pep),np.array(validation_mhc)])\n",
    "    allprobas_ = np.append(allprobas_, probas_)           \n",
    "    allylable = np.append(allylable, validation_target)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font1 = {'family' : 'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "figsize=6.2, 6.2\n",
    "\n",
    "########ROC_figure\n",
    "figure1, ax1 = plt.subplots(figsize=figsize)\n",
    "ax1.tick_params(labelsize=18)\n",
    "labels = ax1.get_xticklabels() + ax1.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]  \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(allylable, allprobas_)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "\n",
    "ax1.plot(fpr, tpr, color='b',\n",
    "    label=r'Mean ROC (AUC = %0.4f)' % (roc_auc),\n",
    "    lw=2, alpha=.8)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Luck', alpha=.8)\n",
    "ax1.set_xlim([-0.05, 1.05])\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate', font1)\n",
    "ax1.set_ylabel('True Positive Rate', font1)\n",
    "title1 = 'Cross Validated ROC Curve'\n",
    "ax1.set_title(title1, font1)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "figure1.savefig(folder + '5_fold_roc.png', dpi=300, bbox_inches = 'tight')\n",
    "\n",
    "########PR_figure\n",
    "figure2, ax2 = plt.subplots(figsize=figsize)\n",
    "ax2.tick_params(labelsize=18)\n",
    "labels = ax2.get_xticklabels() + ax2.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels] \n",
    "\n",
    "precision, recall, _ = precision_recall_curve(allylable, allprobas_)\n",
    "ax2.plot(recall, precision, color='b',\n",
    "        label=r'Precision-Recall (AUC = %0.4f)' % (average_precision_score(allylable, allprobas_)),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "ax2.set_ylim([-0.05, 1.05])\n",
    "ax2.set_xlabel('Recall', font1)\n",
    "ax2.set_ylabel('Precision', font1)\n",
    "title2 = 'Cross Validated PR Curve'\n",
    "ax2.set_title(title2, font1)\n",
    "ax2.legend(loc=\"lower left\")\n",
    "figure2.savefig(folder + '5_fold_pr.png', dpi=300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font1 = {'family' : 'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "figsize=6.2, 6.2\n",
    "\n",
    "########ROC_figure\n",
    "figure1, ax1 = plt.subplots(figsize=figsize)\n",
    "ax1.tick_params(labelsize=18)\n",
    "labels = ax1.get_xticklabels() + ax1.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]  \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, allprobas_)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "\n",
    "ax1.plot(fpr, tpr, color='b',\n",
    "    label=r'ROC (AUC = %0.4f)' % (roc_auc),\n",
    "    lw=2, alpha=.8)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Luck', alpha=.8)\n",
    "ax1.set_xlim([-0.05, 1.05])\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate', font1)\n",
    "ax1.set_ylabel('True Positive Rate', font1)\n",
    "# title1 = 'Cross Validated ROC Curve'\n",
    "# ax1.set_title(title1, font1)\n",
    "ax1.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########PR_figure\n",
    "figure2, ax2 = plt.subplots(figsize=figsize)\n",
    "ax2.tick_params(labelsize=18)\n",
    "labels = ax2.get_xticklabels() + ax2.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels] \n",
    "\n",
    "precision, recall, _ = precision_recall_curve(all_labels, allprobas_)\n",
    "ax2.plot(recall, precision, color='b',\n",
    "        label=r'Precision-Recall (AUC = %0.4f)' % (average_precision_score(all_labels, allprobas_)),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "ax2.set_ylim([-0.05, 1.05])\n",
    "ax2.set_xlabel('Recall', font1)\n",
    "ax2.set_ylabel('Precision', font1)\n",
    "# title2 = 'Cross Validated PR Curve'\n",
    "# ax2.set_title(title2, font1)\n",
    "ax2.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_model.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(saved_model.history[\"loss\"])\n",
    "plt.plot(saved_model.history[\"val_loss\"])\n",
    "plt.legend(['train', 'test'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(saved_model.history[\"accuracy\"])\n",
    "plt.plot(saved_model.history[\"val_accuracy\"])\n",
    "plt.legend(['train', 'test'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('viralepitope': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a569e57c3a868c837518f690d34e8ba538dcefa2ece26ecfaf82d5cd67ebe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
