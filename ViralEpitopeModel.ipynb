{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on August 4th 2022\\n\\n@author: JJ\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on August 4th 2022\n",
    "\n",
    "@author: JJ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 22:51:21.260632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import random\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a well-defined initial state.\n",
    "random.seed\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import (\n",
    "    RMSprop, \n",
    "    SGD, \n",
    "    Adam, \n",
    "    Nadam\n",
    ")\n",
    "from tensorflow.keras.models import (\n",
    "    Sequential, \n",
    "    Model,\n",
    "    load_model\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Dropout, \n",
    "    Activation, \n",
    "    Flatten,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    MaxPooling1D,\n",
    "    MaxPooling2D,\n",
    "    Bidirectional,\n",
    "    Layer\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Input,\n",
    "    concatenate,\n",
    "    multiply,\n",
    "    Reshape,\n",
    "    GRU,\n",
    ")\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from tensorflow.keras.regularizers import (\n",
    "    l2, \n",
    "    l1, \n",
    "    l1_l2\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import (\n",
    "    activations, \n",
    "    initializers, \n",
    "    regularizers, \n",
    "    constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 4\n",
    "# Maximum number of threads to use for OpenMP parallel regions.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# Without setting below 2 environment variables, it didn't work for me. Thanks to @cjw85 \n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"4\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"4\"\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.threading.set_intra_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden,\n",
    "        init=\"glorot_uniform\",\n",
    "        activation=\"linear\",\n",
    "        W_regularizer=None,\n",
    "        b_regularizer=None,\n",
    "        W_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.init = initializers.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.hidden = hidden\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.input_length = input_shape[1]\n",
    "        self.W0 = self.add_weight(\n",
    "            name=\"{}_W1\".format(self.name),\n",
    "            shape=(input_dim, self.hidden),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )  # Keras 2 API\n",
    "        self.W = self.add_weight(\n",
    "            name=\"{}_W\".format(self.name),\n",
    "            shape=(self.hidden, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b0 = K.zeros((self.hidden,), name=\"{}_b0\".format(self.name))\n",
    "        self.b = K.zeros((1,), name=\"{}_b\".format(self.name))\n",
    "        # AttributeError: Can't set the attribute \"trainable_weights\",\n",
    "        # likely because it conflicts with an existing read-only @property of the object.\n",
    "        # Please choose a different name.\n",
    "        # https://issueexplorer.com/issue/wenguanwang/ASNet/8\n",
    "        self._trainable_weights = [self.W0, self.W, self.b, self.b0]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.b_regularizer:\n",
    "            self.b_regularizer.set_param(self.b)\n",
    "            self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "        self.constraints = {}\n",
    "        if self.W_constraint:\n",
    "            self.constraints[self.W0] = self.W_constraint\n",
    "            self.constraints[self.W] = self.W_constraint\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attmap = self.activation(K.dot(x, self.W0) + self.b0)\n",
    "        attmap = K.dot(attmap, self.W) + self.b\n",
    "        attmap = K.reshape(\n",
    "            attmap, (-1, self.input_length)\n",
    "        )  # Softmax needs one dimension\n",
    "        attmap = K.softmax(attmap)\n",
    "        dense_representation = K.batch_dot(attmap, x, axes=(1, 1))\n",
    "        out = K.concatenate(\n",
    "            [dense_representation, attmap]\n",
    "        )  # Output the attention maps but do not pass it to the next layer by DIY flatten layer\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1] + input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"init\": \"glorot_uniform\",\n",
    "            \"activation\": self.activation.__name__,\n",
    "            \"W_constraint\": self.W_constraint.get_config()\n",
    "            if self.W_constraint\n",
    "            else None,\n",
    "            \"W_regularizer\": self.W_regularizer.get_config()\n",
    "            if self.W_regularizer\n",
    "            else None,\n",
    "            \"b_regularizer\": self.b_regularizer.get_config()\n",
    "            if self.b_regularizer\n",
    "            else None,\n",
    "            \"hidden\": self.hidden if self.hidden else None,\n",
    "        }\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class attention_flatten(Layer):  # Based on the source code of Keras flatten\n",
    "    def __init__(self, keep_dim, **kwargs):\n",
    "        self.keep_dim = keep_dim # 64\n",
    "        super(attention_flatten, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not all(input_shape[1:]):\n",
    "            raise Exception(\n",
    "                'The shape of the input to \"Flatten\" '\n",
    "                \"is not fully defined \"\n",
    "                \"(got \" + str(input_shape[2:]) + \". \"\n",
    "                'Make sure to pass a complete \"input_shape\" '\n",
    "                'or \"batch_input_shape\" argument to the first '\n",
    "                \"layer in your model.\"\n",
    "            )\n",
    "        return (input_shape[0], self.keep_dim)  # Remove the attention map\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = x[:, : self.keep_dim]\n",
    "        #return K.batch_flatten(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    chi2\n",
    ")\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('fulldata_pseudoseq_dictionary.pkl', 'rb') as f:\n",
    "    pseudo_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pseudo_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227145/3434162491.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  allele_data = np.array(allele_data)\n"
     ]
    }
   ],
   "source": [
    "    training_data = []\n",
    "    for allele in pseudo_seq.keys():\n",
    "        allele_data = pseudo_seq[allele]\n",
    "        random.shuffle(allele_data)\n",
    "        allele_data = np.array(allele_data)\n",
    "        training_data.extend(allele_data)\n",
    "    \n",
    "    [all_pep, all_mhc, all_target] = [[i[j] for i in training_data] for j in range(3)]\n",
    "    all_pep = np.array(all_pep)\n",
    "    all_mhc = np.array(all_mhc)\n",
    "    all_target = np.array(all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(945434, 30, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1337)    \n",
    "allprobas_=np.array([]) \n",
    "all_labels=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(filter = 256, kernel = 9, kernel_constraint = 3,\n",
    "\t\t\t\tactivation = 'relu', pool = 3, dropout1 = 0.6, \n",
    "\t\t\t\tdropout2 = 0.5, lr = 0.001):\n",
    "\n",
    "\t\t\t\tinput1 = Input(shape = (30,20), name = 'peptide')\n",
    "\n",
    "\t\t\t\tconv1  = Conv1D(filters = filter,\n",
    "\t\t\t\t\tkernel_size = kernel,\n",
    "\t\t\t\t\tpadding = 'same', \n",
    "\t\t\t\t\tactivation = activation, \n",
    "\t\t\t\t\tkernel_constraint = MaxNorm(kernel_constraint), name = 'conv1')\n",
    "\n",
    "\t\t\t\tconv1_pool_out = MaxPooling1D(pool_size = pool)\n",
    "\n",
    "\t\t\t\tconv1_pool_dropout_out = Dropout(dropout1)\n",
    "\t\t\t\tconv1_pool_dropout2_out = Dropout(dropout2)\n",
    "\n",
    "\t\t\t\tdecoder  = Attention(hidden = 256, activation = 'linear')\n",
    "\t\t\t\tdense1   = Dense(1)\n",
    "\n",
    "\t\t\t\toutput_1 = conv1(input1)\n",
    "\t\t\t\toutput_1b = conv1_pool_out(output_1)\n",
    "\t\t\t\toutput_2 = conv1_pool_dropout_out(output_1b)\n",
    "\n",
    "\t\t\t\tatt_decoder = decoder(output_2)\n",
    "\t\t\t\toutput_3 = attention_flatten(output_2.shape[2])(att_decoder)\n",
    "\n",
    "\t\t\t\toutput_4 =  dense1(conv1_pool_dropout2_out(Flatten()(output_2)))\n",
    "\n",
    "\t\t\t\toutput_all = concatenate([output_3, output_4])\n",
    "\n",
    "\t\t\t\toutput_5 =  Dense(1)(output_all)\n",
    "\t\t\t\toutput_f =  Activation('sigmoid')(output_5)\n",
    "\n",
    "\n",
    "\t\t\t\tmodel = Model(inputs = input1, outputs = output_f)\n",
    "\t\t\t\topt = Adam(learning_rate= lr)\n",
    "\t\t\t\tmodel.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy', \"AUC\"])\n",
    "\t\t\t\tmodel.summary()\n",
    "\t\t\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in enumerate(kfold.split(all_pep, all_target)):\n",
    "    training_pep = all_pep[train]\n",
    "    training_mhc = all_mhc[train]\n",
    "    training_target = all_target[train]\n",
    "    \n",
    "    validation_pep = all_pep[test]\n",
    "    validation_mhc = all_mhc[test]\n",
    "    validation_target = all_target[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:204 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_2 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 30, 20) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=string>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m saved_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit((training_pep,training_mhc), \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m             training_target,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m             shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             callbacks\u001b[39m=\u001b[39;49m[es],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             validation_data\u001b[39m=\u001b[39;49m((validation_pep,validation_mhc), validation_target),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m probas_ \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mpredict([np\u001b[39m.\u001b[39marray(validation_pep),np\u001b[39m.\u001b[39marray(validation_mhc)])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.106.31.44/home/jjia1/viralepitope/viralepitope/ViralEpitopeModel.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m allprobas_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(allprobas_, probas_)           \n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1101\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    829\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:871\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 871\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    872\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:725\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    723\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    724\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn\u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    726\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    728\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    729\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2969\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2969\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2970\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3361\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3358\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> 3361\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   3362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[1;32m   3364\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3196\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3191\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   3192\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3193\u001b[0m ]\n\u001b[1;32m   3194\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   3195\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   3197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   3198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   3199\u001b[0m         args,\n\u001b[1;32m   3200\u001b[0m         kwargs,\n\u001b[1;32m   3201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   3202\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   3203\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   3204\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   3205\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[1;32m   3206\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   3207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   3208\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   3209\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3211\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3214\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:990\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> 990\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m    992\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m    995\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:634\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m     xla_context\u001b[39m.\u001b[39mExit()\n\u001b[1;32m    633\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m   out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:977\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    976\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 977\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    978\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/jjia1/miniconda3/envs/viralepitope/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:204 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_2 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 30, 20) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=string>]\n"
     ]
    }
   ],
   "source": [
    "saved_model = model.fit([training_pep,training_mhc], \n",
    "            training_target,\n",
    "            batch_size=512,\n",
    "            epochs = 100,\n",
    "            shuffle=True,\n",
    "            callbacks=[es],\n",
    "            validation_data=([validation_pep,validation_mhc], validation_target),\n",
    "            verbose=1)\n",
    "\n",
    "probas_ = saved_model.predict([np.array(validation_pep),np.array(validation_mhc)])\n",
    "allprobas_ = np.append(allprobas_, probas_)           \n",
    "all_labels = np.append(all_labels, validation_target)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('viralepitope': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a569e57c3a868c837518f690d34e8ba538dcefa2ece26ecfaf82d5cd67ebe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
