{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on August 4th 2022\\n\\n@author: jjia1\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on August 4th 2022\n",
    "\n",
    "@author: JJ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 16:51:34.501450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import random\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a well-defined initial state.\n",
    "random.seed\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import (\n",
    "    RMSprop, \n",
    "    SGD, \n",
    "    Adam, \n",
    "    Nadam\n",
    ")\n",
    "from tensorflow.keras.models import (\n",
    "    Sequential, \n",
    "    Model,\n",
    "    load_model\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Dropout, \n",
    "    Activation, \n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D,\n",
    "    Bidirectional,\n",
    "    Layer\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Input,\n",
    "    concatenate,\n",
    "    multiply,\n",
    "    Reshape,\n",
    "    GRU,\n",
    ")\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from tensorflow.keras.regularizers import (\n",
    "    l2, \n",
    "    l1, \n",
    "    l1_l2\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import (\n",
    "    activations, \n",
    "    initializers, \n",
    "    regularizers, \n",
    "    constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 4\n",
    "# Maximum number of threads to use for OpenMP parallel regions.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# Without setting below 2 environment variables, it didn't work for me. Thanks to @cjw85 \n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"4\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"4\"\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.threading.set_intra_op_parallelism_threads(\n",
    "    num_threads\n",
    ")\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden,\n",
    "        init=\"glorot_uniform\",\n",
    "        activation=\"linear\",\n",
    "        W_regularizer=None,\n",
    "        b_regularizer=None,\n",
    "        W_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.init = initializers.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.hidden = hidden\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.input_length = input_shape[1]\n",
    "        self.W0 = self.add_weight(\n",
    "            name=\"{}_W1\".format(self.name),\n",
    "            shape=(input_dim, self.hidden),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )  # Keras 2 API\n",
    "        self.W = self.add_weight(\n",
    "            name=\"{}_W\".format(self.name),\n",
    "            shape=(self.hidden, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b0 = K.zeros((self.hidden,), name=\"{}_b0\".format(self.name))\n",
    "        self.b = K.zeros((1,), name=\"{}_b\".format(self.name))\n",
    "        # AttributeError: Can't set the attribute \"trainable_weights\",\n",
    "        # likely because it conflicts with an existing read-only @property of the object.\n",
    "        # Please choose a different name.\n",
    "        # https://issueexplorer.com/issue/wenguanwang/ASNet/8\n",
    "        self._trainable_weights = [self.W0, self.W, self.b, self.b0]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.b_regularizer:\n",
    "            self.b_regularizer.set_param(self.b)\n",
    "            self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "        self.constraints = {}\n",
    "        if self.W_constraint:\n",
    "            self.constraints[self.W0] = self.W_constraint\n",
    "            self.constraints[self.W] = self.W_constraint\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attmap = self.activation(K.dot(x, self.W0) + self.b0)\n",
    "        attmap = K.dot(attmap, self.W) + self.b\n",
    "        attmap = K.reshape(\n",
    "            attmap, (-1, self.input_length)\n",
    "        )  # Softmax needs one dimension\n",
    "        attmap = K.softmax(attmap)\n",
    "        dense_representation = K.batch_dot(attmap, x, axes=(1, 1))\n",
    "        out = K.concatenate(\n",
    "            [dense_representation, attmap]\n",
    "        )  # Output the attention maps but do not pass it to the next layer by DIY flatten layer\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1] + input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"init\": \"glorot_uniform\",\n",
    "            \"activation\": self.activation.__name__,\n",
    "            \"W_constraint\": self.W_constraint.get_config()\n",
    "            if self.W_constraint\n",
    "            else None,\n",
    "            \"W_regularizer\": self.W_regularizer.get_config()\n",
    "            if self.W_regularizer\n",
    "            else None,\n",
    "            \"b_regularizer\": self.b_regularizer.get_config()\n",
    "            if self.b_regularizer\n",
    "            else None,\n",
    "            \"hidden\": self.hidden if self.hidden else None,\n",
    "        }\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class attention_flatten(Layer):  # Based on the source code of Keras flatten\n",
    "    def __init__(self, keep_dim, **kwargs):\n",
    "        self.keep_dim = keep_dim # 64\n",
    "        super(attention_flatten, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not all(input_shape[1:]):\n",
    "            raise Exception(\n",
    "                'The shape of the input to \"Flatten\" '\n",
    "                \"is not fully defined \"\n",
    "                \"(got \" + str(input_shape[2:]) + \". \"\n",
    "                'Make sure to pass a complete \"input_shape\" '\n",
    "                'or \"batch_input_shape\" argument to the first '\n",
    "                \"layer in your model.\"\n",
    "            )\n",
    "        return (input_shape[0], self.keep_dim)  # Remove the attention map\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = x[:, : self.keep_dim]\n",
    "        #return K.batch_flatten(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    chi2\n",
    ")\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('/home/jjia1/jjia1/viralepitope/pos_pseudo_seq_dictionary.pkl', 'rb') as f:\n",
    "    positive_pseudo_seq_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open ('/home/jjia1/jjia1/viralepitope/neg_pseudo_seq_dictionary.pkl', 'rb')) as f:\n",
    "    negative_pseudo_seq_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positive_pseudo_seq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(negative_pseudo_seq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)\n",
    "    input = Input(  )\n",
    "    conv = Conv2D()\n",
    "    pool = MaxPooling2D()\n",
    "    dense1 = Dense()\n",
    "    \n",
    "    model.compile()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('viralepitope': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a569e57c3a868c837518f690d34e8ba538dcefa2ece26ecfaf82d5cd67ebe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
